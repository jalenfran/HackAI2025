{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starter Code\n",
    "\n",
    "<span style=\"color:blue\">**In this guide, we provide a starter code to help you begin your project. \n",
    "Please feel free to use it thoughtfully and tailor it to your specific requirements.**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "**Facial Recognition**\n",
    "\n",
    "Dataset: FER2013\n",
    "- CSV Format: Kaggle Link https://www.kaggle.com/datasets/nicolejyt/facialexpressionrecognition\n",
    "- JPG Format: Kaggle Link https://www.kaggle.com/datasets/msambare/fer2013/data\n",
    "- Details:\n",
    "    - 34,034 images\n",
    "    - 48x48 pixels\n",
    "    - 7 emotions: Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral\n",
    "\n",
    "**Gesture Recognition**\n",
    "\n",
    "Dataset: LeapGestureRecognition\n",
    "\n",
    "- Download Link: Kaggle https://www.kaggle.com/datasets/gti-upm/leapgestrecog\n",
    "- Other Gesture Dataset: GitHub https://github.com/linto-ai/multi-hand-gesture-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples & Inspirations:\n",
    "\n",
    "#### Facial Expression\n",
    "\n",
    "1. Static and dynamic facial emotion recognition using the Emo-AffectNet \n",
    "\n",
    "   https://huggingface.co/ElenaRyumina/face_emotion_recognition\n",
    "\n",
    "\n",
    "   ![example](https://github.com/ElenaRyumina/EMO-AffectNetModel/blob/main/gif/result_2.gif?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Facial Expression Recognition Challenge\n",
    "\n",
    "   https://github.com/chinhau-lim/fer_2013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hand Gesture\n",
    "\n",
    "1. Real-time hand gesture recognition using TensorFlow & OpenCV\n",
    "\n",
    "   https://techvidvan.com/tutorials/hand-gesture-recognition-tensorflow-opencv/\n",
    "\n",
    "    ![example](https://techvidvan.com/tutorials/wp-content/uploads/sites/2/2021/07/landmark-output.gif)\n",
    "\n",
    "\n",
    "\n",
    "2. Deep_learning_hand_gesture_recognition\n",
    "\n",
    "   https://github.com/guillaumephd/deep_learning_hand_gesture_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip3 install opencv-python numpy pandas tensorflow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facial Emotion Classes\n",
    "EMOTION_MAP = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# Gesture Classes (LeapGestRecog)\n",
    "GESTURE_MAP = {\n",
    "    0: 'palm',        # 01_palm\n",
    "    1: 'l',           # 02_l\n",
    "    2: 'fist',        # 03_fist\n",
    "    3: 'fist_moved',  # 04_fist_moved\n",
    "    4: 'thumb',       # 05_thumb\n",
    "    5: 'index',       # 06_index\n",
    "    6: 'ok',          # 07_ok\n",
    "    7: 'palm_moved',  # 08_palm_moved\n",
    "    8: 'c',           # 09_c\n",
    "    9: 'down'         # 10_down\n",
    "}\n",
    "\n",
    "def load_fer2013(file_path='fer2013.csv'):\n",
    "    \"\"\"Loads and balances FER2013 dataset\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    pixels = df['pixels'].apply(lambda x: np.array(x.split(), dtype='float32'))\n",
    "    images = np.array([x.reshape(48, 48, 1) for x in pixels]) / 255.0\n",
    "    labels = df['emotion'].values\n",
    "    \n",
    "    # Handle class imbalance\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "    return images, labels, dict(enumerate(class_weights))\n",
    "\n",
    "def load_leap_gestures(dataset_path=\"LeapGestRecog\"):\n",
    "    \"\"\"Loads Leap Motion dataset with correct label mapping\"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    for root, _, files in os.walk(dataset_path):\n",
    "        try:\n",
    "            folder_name = os.path.basename(root)\n",
    "            if not folder_name[:2].isdigit():\n",
    "                continue\n",
    "                \n",
    "            gesture_num = int(folder_name[:2]) - 1\n",
    "            if gesture_num not in GESTURE_MAP:\n",
    "                continue\n",
    "                \n",
    "            # Process images\n",
    "            for file in files:\n",
    "                if file.endswith(\".png\"):\n",
    "                    img = cv2.imread(os.path.join(root, file), cv2.IMREAD_GRAYSCALE)\n",
    "                    img = cv2.resize(img, (64, 64))\n",
    "                    X.append(img.astype('float32') / 255.0)\n",
    "                    y.append(gesture_num)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {root}: {str(e)}\")\n",
    "    \n",
    "    return np.array(X).reshape(-1, 64, 64, 1), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotiveAvatarSystem:\n",
    "    def __init__(self):\n",
    "        self.face_model = self.build_face_model()\n",
    "        self.gesture_model = self.build_gesture_model()\n",
    "        \n",
    "    def build_face_model(self):\n",
    "        model = models.Sequential([\n",
    "            layers.Conv2D(32, (3,3), activation='relu', input_shape=(48,48,1)),\n",
    "            layers.MaxPooling2D((2,2)),\n",
    "            layers.Dropout(0.3),\n",
    "            \n",
    "            layers.Conv2D(64, (3,3), activation='relu'),\n",
    "            layers.MaxPooling2D((2,2)),\n",
    "            layers.Dropout(0.4),\n",
    "            \n",
    "            layers.GlobalAveragePooling2D(),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(7, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer='adam',\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def build_gesture_model(self):\n",
    "        model = models.Sequential([\n",
    "            layers.Conv2D(16, (3,3), activation='relu', input_shape=(64,64,1)),\n",
    "            layers.MaxPooling2D((2,2)),\n",
    "            layers.Dropout(0.25),\n",
    "            \n",
    "            layers.Conv2D(32, (3,3), activation='relu'),\n",
    "            layers.MaxPooling2D((2,2)),\n",
    "            layers.Dropout(0.35),\n",
    "            \n",
    "            layers.GlobalAveragePooling2D(),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer='adam',\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/duan.425/Library/Python/3.10/lib/python/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Facial Model (FER2013)...\n",
      "Epoch 1/5\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.1265 - loss: 1.9515 - val_accuracy: 0.1855 - val_loss: 1.9190 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.1673 - loss: 1.9090 - val_accuracy: 0.1804 - val_loss: 1.9212 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.1765 - loss: 1.9311 - val_accuracy: 0.1942 - val_loss: 1.8988 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.2010 - loss: 1.8664 - val_accuracy: 0.1296 - val_loss: 1.9502 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m359/359\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step - accuracy: 0.1636 - loss: 1.8952 - val_accuracy: 0.1827 - val_loss: 1.9028 - learning_rate: 0.0010\n",
      "\n",
      "Training Gesture Model (Leap Motion)...\n",
      "Epoch 1/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.1105 - loss: 2.3009 - val_accuracy: 0.0938 - val_loss: 2.2951\n",
      "Epoch 2/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1072 - loss: 2.2760 - val_accuracy: 0.0938 - val_loss: 2.2756\n",
      "Epoch 3/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1227 - loss: 2.2387 - val_accuracy: 0.0938 - val_loss: 2.2185\n",
      "Epoch 4/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1715 - loss: 2.1500 - val_accuracy: 0.2125 - val_loss: 2.0492\n",
      "Epoch 5/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.2546 - loss: 1.9724 - val_accuracy: 0.2188 - val_loss: 1.8632\n",
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.1892 - loss: 1.8973\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2111 - loss: 1.8481 \n",
      "\n",
      "=== Final Performance ===\n",
      "Facial Accuracy: 18.95%\n",
      "Gesture Accuracy: 23.00%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    X_face, y_face, face_weights = load_fer2013()\n",
    "    X_gesture, y_gesture = load_leap_gestures()\n",
    "    \n",
    "    # Split datasets\n",
    "    Xf_train, Xf_test, yf_train, yf_test = train_test_split(X_face, y_face, test_size=0.2, stratify=y_face)\n",
    "    Xg_train, Xg_test, yg_train, yg_test = train_test_split(X_gesture, y_gesture, test_size=0.2, stratify=y_gesture)\n",
    "    \n",
    "    # Initialize system\n",
    "    avatar = EmotiveAvatarSystem()\n",
    "    \n",
    "    # Train facial model\n",
    "    print(\"Training Facial Model (FER2013)...\")\n",
    "    avatar.face_model.fit(Xf_train, yf_train,\n",
    "                         epochs=5,\n",
    "                         batch_size=64,\n",
    "                         class_weight=face_weights,\n",
    "                         validation_split=0.2,\n",
    "                         callbacks=[\n",
    "                             callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "                             callbacks.ReduceLROnPlateau(factor=0.5, patience=2)\n",
    "                         ])\n",
    "    \n",
    "    # Train gesture model\n",
    "    print(\"\\nTraining Gesture Model (Leap Motion)...\")\n",
    "    avatar.gesture_model.fit(Xg_train, yg_train,\n",
    "                            epochs=5,\n",
    "                            batch_size=32,\n",
    "                            validation_split=0.2,\n",
    "                            callbacks=[\n",
    "                                callbacks.EarlyStopping(patience=3)\n",
    "                            ])\n",
    "    \n",
    "    # Evaluate\n",
    "    f_loss, f_acc = avatar.face_model.evaluate(Xf_test, yf_test)\n",
    "    g_loss, g_acc = avatar.gesture_model.evaluate(Xg_test, yg_test)\n",
    "    \n",
    "    print(\"\\n=== Final Performance ===\")\n",
    "    print(f\"Facial Accuracy: {f_acc:.2%}\")\n",
    "    print(f\"Gesture Accuracy: {g_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement Opportunities:\n",
    "1. Add real-time webcam integration\n",
    "2. Implement data augmentation for infrared images\n",
    "4. Add attention mechanisms to the CNN\n",
    "5. ......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "https://github.com/chinhau-lim/fer_2013 \n",
    "\n",
    "https://arxiv.org/pdf/2105.03588\n",
    "\n",
    "https://github.com/takanto/FER2013\n",
    "\n",
    "https://aicompetence.org/ai-in-cultural-sensitivity-gesture-interpretation/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
